from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch
from config import debug_mode

# Change the model by editing the string
MODEL_NAME = "EleutherAI/gpt-neo-1.3B"

class ModelHandler:
    def __init__(self):
        """Initialize the tokenizer and model."""
        if debug_mode:
            print("Loading tokenizer and model...")

        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True
        )

        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            device_map="auto",
            quantization_config=quantization_config,
        )

        if debug_mode:
            print("Tokenizer and model loaded successfully.")

    def generate_response(self, prompt: str, max_length: int = 200) -> str:
        """Generate a response to the given user prompt.

        Args:
            prompt (str): The user's input prompt.
            max_length (int): The maximum length of the generated response.

        Returns:
            str: The response generated by the model.
        """
        if debug_mode:
            print(f"Processing prompt: {prompt}")

        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True).to("cuda")

        if "attention_mask" not in inputs:
            inputs["attention_mask"] = torch.ones_like(inputs["input_ids"])

        outputs = self.model.generate(
            inputs.input_ids,
            attention_mask=inputs["attention_mask"],
            max_length=max_length,
            num_return_sequences=1,
            do_sample=True,
            temperature=0.7,
        )

        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        if debug_mode:
            print("Response generated successfully.")
        return response

if __name__ == "__main__":
    handler = ModelHandler()
    user_prompt = "Tell me about the highest mountain in the world."
    response = handler.generate_response(user_prompt)
    print("Model Response:", response)
